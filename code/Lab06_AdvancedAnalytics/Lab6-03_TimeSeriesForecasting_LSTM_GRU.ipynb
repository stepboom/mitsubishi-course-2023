{"cells":[{"cell_type":"markdown","metadata":{"id":"OefjyxueoGsK"},"source":["# How to use PyTorch LSTMs/GRUs for time series regression\n","We aim to forcast stock price value by using LSTM with Pytorch.\n","\n","credit: https://github.com/CrosstabKite/lstm-forecasting/blob/master/lstm_forecasting.ipynb\n","\n","modified on (12/02/2023)"]},{"cell_type":"markdown","metadata":{"id":"roINhp6eoGsL"},"source":["# Data"]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"id":"3beiQV4ETScE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://github.com/stepboom/mitsubishi-course-2023/raw/main/data/GOOG.csv"],"metadata":{"id":"AnbxcBiPBVGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cR4-kxSHoGsV"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv('GOOG.csv', index_col=\"Date\")\n","df = df.drop(['Adj Close'], axis = 1)\n","df"]},{"cell_type":"code","source":["df.loc['2019-02-26']"],"metadata":{"id":"Knhw8Jebskjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.express as px\n","import plotly.graph_objects as go\n","import plotly.io as pio\n","pio.templates.default = \"plotly_white\"\n","\n","plot_template = dict(\n","    layout=go.Layout({\n","        \"font_size\": 18,\n","        \"xaxis_title_font_size\": 24,\n","        \"yaxis_title_font_size\": 24})\n",")\n","\n","fig = px.line(df['Open'], labels=dict(\n","    created_at=\"Date\", value=\"Open\", variable=\"Sensor\"\n","))\n","fig.update_layout(\n","  template=plot_template, legend=dict(orientation='h', y=1.02, title_text=\"\")\n",")\n","fig.show()"],"metadata":{"id":"eKO5UE4BD7g7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xz0kYc8uoGsh"},"source":["## Create the target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uELZWZVHoGsh"},"outputs":[],"source":["target_col = \"Open\"\n","features = list(df.columns.difference([target_col]))\n","\n","forecast_lead = 1\n","target = f\"{target_col}_lead{forecast_lead}\"\n","\n","df[target] = df[target_col].shift(-forecast_lead)\n","df = df.iloc[:-forecast_lead]"]},{"cell_type":"code","source":["df"],"metadata":{"id":"Bmd9FOW4CPVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCfYZ_lGoGsi"},"source":["## Create a hold-out test set and preprocess the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBXPBOqroGsi"},"outputs":[],"source":["test_start = \"2019-01-01\"\n","val_start = \"2018-01-01\"\n","\n","df_train = df.loc[:val_start].copy()\n","df_val = df.loc[val_start:test_start].copy()\n","df_test = df.loc[test_start:].copy()\n","\n","print(\"Test set fraction:\", len(df_test) / len(df))"]},{"cell_type":"markdown","metadata":{"id":"5646NJnGoGsi"},"source":["## Standardize the features and target, based on the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTVap0kkoGsi"},"outputs":[],"source":["target_mean = df_train[target].mean()\n","target_stdev = df_train[target].std()\n","\n","for c in df_train.columns:\n","    mean = df_train[c].mean()\n","    stdev = df_train[c].std()\n","\n","    df_train[c] = (df_train[c] - mean) / stdev\n","    df_val[c] = (df_val[c] - mean) / stdev\n","    df_test[c] = (df_test[c] - mean) / stdev"]},{"cell_type":"markdown","metadata":{"id":"8SjVLo7FoGsi"},"source":["## Create datasets that PyTorch `DataLoader` can work with"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2B_x4fHZoGsj"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class SequenceDataset(Dataset):\n","    def __init__(self, dataframe, target, features, sequence_length=5):\n","        self.features = features\n","        self.target = target\n","        self.sequence_length = sequence_length\n","        self.y = torch.tensor(dataframe[self.target].values).float()\n","        self.X = torch.tensor(dataframe[self.features].values).float()\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, i): \n","        if i >= self.sequence_length - 1:\n","            i_start = i - self.sequence_length + 1\n","            x = self.X[i_start:(i + 1), :]\n","        else:\n","            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n","            x = self.X[0:(i + 1), :]\n","            x = torch.cat((padding, x), 0)\n","\n","        return x, self.y[i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wu5I547IoGsj"},"outputs":[],"source":["i = 27\n","sequence_length = 4\n","\n","train_dataset = SequenceDataset(\n","    df_train,\n","    target=target,\n","    features=features,\n","    sequence_length=sequence_length\n",")\n","\n","X, y = train_dataset[i]\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZuBGHQhoGsj"},"outputs":[],"source":["X, y = train_dataset[i + 1]\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrxdhyJRoGsk"},"outputs":[],"source":["print(df_train[features].iloc[(i - sequence_length + 1): (i + 1)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRBl-HIGoGsk"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","torch.manual_seed(99)\n","\n","train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n","\n","X, y = next(iter(train_loader))\n","print(X.shape)\n","print(X)"]},{"cell_type":"markdown","metadata":{"id":"NzQKgf5RoGsk"},"source":["## Create the datasets and data loaders for real"]},{"cell_type":"markdown","metadata":{"id":"qi0vsoH1oGsk"},"source":["In this tutorial we will\n","use sequences of length 60 (60 days) to forcast 1 day ahead.\n","\n","The PyTorch `DataLoader` is a very convenient way to iterate through these datasets. For\n","the training set we'll shuffle (the rows *within* each training sequence are not\n","shuffled, only the order in which we draw those blocks). For the test set, shuffling\n","isn't necessary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IpeaDGKoGsl"},"outputs":[],"source":["torch.manual_seed(101)\n","\n","batch_size = 32\n","sequence_length = 60\n","\n","train_dataset = SequenceDataset(\n","    df_train,\n","    target=target,\n","    features=features,\n","    sequence_length=sequence_length\n",")\n","val_dataset = SequenceDataset(\n","    df_val,\n","    target=target,\n","    features=features,\n","    sequence_length=sequence_length\n",")\n","test_dataset = SequenceDataset(\n","    df_test,\n","    target=target,\n","    features=features,\n","    sequence_length=sequence_length\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","X, y = next(iter(train_loader))\n","\n","print(\"Features shape:\", X.shape)\n","print(\"Target shape:\", y.shape)"]},{"cell_type":"markdown","source":["# LSTM"],"metadata":{"id":"w__P7b2vL2-S"}},{"cell_type":"markdown","metadata":{"id":"u9qjwSZGoGsl"},"source":["## The model and learning algorithm"]},{"cell_type":"markdown","source":["![picture](https://i.stack.imgur.com/SjnTl.png)\n","\n","Credit : https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm "],"metadata":{"id":"jAI32yVsrGog"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"dB-LjIMXYuG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEm8dn6yoGsl"},"outputs":[],"source":["from torch import nn\n","\n","class ShallowRegressionLSTM(nn.Module):\n","    def __init__(self, num_features, hidden_units):\n","        super().__init__()\n","        self.num_features = num_features  # this is the number of features\n","        self.hidden_units = hidden_units \n","        self.num_layers = 4\n","\n","        self.lstm = nn.LSTM(\n","            input_size=num_features,\n","            hidden_size=hidden_units,\n","            batch_first=True,\n","            num_layers=self.num_layers\n","        )\n","\n","        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","\n","        # initialize the hidden and cell state of the LSTM layer\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).to(device).requires_grad_()\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).to(device).requires_grad_()\n","        \n","        _, (hn, _) = self.lstm(x, (h0, c0))\n","        out = self.linear(hn[-1]).flatten()  # get the output of the last hidden layer\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsAE3hw4oGsn"},"outputs":[],"source":["learning_rate = 5e-4\n","num_hidden_units = 60\n","\n","model = ShallowRegressionLSTM(num_features=len(features), hidden_units=num_hidden_units)\n","model.to(device)\n","loss_function = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","source":["from torchinfo import summary\n","summary(model, input_size=(32, 60, 4))"],"metadata":{"id":"PiRYwqeOTWTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWoZlk43oGsn"},"source":["## Train"]},{"cell_type":"code","source":["from tqdm.notebook import tqdm"],"metadata":{"id":"8dogVellgMdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EVJhpSnoGso"},"outputs":[],"source":["def train_model(data_loader, model, loss_function, optimizer):\n","    num_batches = len(data_loader)\n","    total_loss = 0\n","    model.train()\n","    \n","    for X, y in data_loader:\n","        X = X.to(device)\n","        y = y.to(device)\n","        output = model(X)\n","        loss = loss_function(output, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / num_batches\n","    print(f\"Train loss: {avg_loss}\")\n","\n","def test_model(data_loader, model, loss_function, best_val_loss):\n","    \n","    num_batches = len(data_loader)\n","    total_loss = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in data_loader:\n","            X = X.to(device)\n","            y = y.to(device)\n","            output = model(X)\n","            total_loss += loss_function(output, y).item()\n","\n","    avg_loss = total_loss / num_batches\n","    print(f\"Test loss: {avg_loss}\")\n","    if avg_loss < best_val_loss:\n","        best_val_loss = avg_loss\n","        torch.save(model.state_dict(), 'model.pth')\n","        print('Save new best model')\n","    return best_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RwmIPZToGsp"},"outputs":[],"source":["best_val_loss = torch.inf\n","for ix_epoch in tqdm(range(100)):\n","    print(f\"Epoch {ix_epoch}\\n---------\")\n","    train_model(train_loader, model, loss_function, optimizer=optimizer)\n","    best_val_loss = test_model(val_loader, model, loss_function, best_val_loss)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"C6DYsOsuoGsp"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0Y-4tuvoGsp"},"outputs":[],"source":["def predict(data_loader, model):\n","    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n","    function.\n","    \"\"\"\n","    output = torch.tensor([])\n","    model.eval()\n","    with torch.no_grad():\n","        for X, _ in data_loader:\n","            X = X.to(device)\n","            y_star = model(X)\n","            output = torch.cat((output, y_star.detach().cpu()), 0)\n","    \n","    return output"]},{"cell_type":"code","source":["PATH = './model.pth'\n","model.load_state_dict(torch.load(PATH))"],"metadata":{"id":"aZn84iyxVaPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"541fAaonoGsp"},"outputs":[],"source":["train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","\n","ystar_col = \"Model forecast\"\n","df_train[ystar_col] = predict(train_eval_loader, model).numpy()\n","df_val[ystar_col] = predict(val_loader, model).numpy()\n","df_test[ystar_col] = predict(test_loader, model).numpy()\n","\n","df_out = pd.concat((df_train, df_val, df_test))[[target, ystar_col]]\n","\n","for c in df_out.columns:\n","    df_out[c] = df_out[c] * target_stdev + target_mean\n","\n","print(df_out)"]},{"cell_type":"code","source":["import numpy as np\n","import math\n","from sklearn.metrics import mean_squared_error\n","\n","def MAPE(Y_actual,Y_Predicted):\n","    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n","    return mape\n","\n","print( 'MPAE =', MAPE(df_test['Open_lead1'], df_test['Model forecast']) )\n","print( 'RMSE =', math.sqrt(mean_squared_error(df_test['Open_lead1'], df_test['Model forecast'])) )"],"metadata":{"id":"bzH9PG_attOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGZQ5iiHoGsq"},"outputs":[],"source":["fig = px.line(df_out, labels={'value': \"Open\", 'created_at': 'Date'})\n","fig.add_vline(x=val_start, line_width=4, line_dash=\"dash\")\n","fig.add_vline(x=test_start, line_width=4, line_dash=\"dash\")\n","# fig.add_annotation(xref=\"paper\", x=0.75, yref=\"paper\", y=0.8, text=\"Test set start\", showarrow=False)\n","fig.update_layout(\n","  template=plot_template, legend=dict(orientation='h', y=1.02, title_text=\"\")\n",")\n","fig.show()"]},{"cell_type":"markdown","source":["# GRU"],"metadata":{"id":"dQwyK33bJFSH"}},{"cell_type":"markdown","source":["## The model and learning algorithm"],"metadata":{"id":"AHAw52RVMGyH"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"vMSusByVJTJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jweI4U_1JTJH"},"outputs":[],"source":["from torch import nn\n","\n","class ShallowRegressionGRU(nn.Module):\n","    def __init__(self, num_features, hidden_units):\n","        super().__init__()\n","        self.num_features = num_features  # this is the number of features\n","        self.hidden_units = hidden_units \n","        self.num_layers = 4\n","\n","        self.gru = nn.GRU(\n","            input_size=num_features,\n","            hidden_size=hidden_units,\n","            batch_first=True,\n","            num_layers=self.num_layers\n","        )\n","\n","        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","\n","        # initialize the hidden and cell state of the LSTM layer\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).to(device).requires_grad_()\n","        \n","        _, hn = self.gru(x, h0)\n","        out = self.linear(hn[-1]).flatten()  # get the output of the last hidden layer\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viPODmc0JTJI"},"outputs":[],"source":["learning_rate = 5e-4\n","num_hidden_units = 60\n","\n","model = ShallowRegressionGRU(num_features=len(features), hidden_units=num_hidden_units)\n","model.to(device)\n","loss_function = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","source":["from torchinfo import summary\n","summary(model, input_size=(32, 60, 4))"],"metadata":{"id":"glqwFIk4JTJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jW7NS5hkJTJJ"},"source":["## Train"]},{"cell_type":"code","source":["from tqdm.notebook import tqdm"],"metadata":{"id":"CF3vVwSpJTJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cl30YYaJTJJ"},"outputs":[],"source":["def train_model(data_loader, model, loss_function, optimizer):\n","    num_batches = len(data_loader)\n","    total_loss = 0\n","    model.train()\n","    \n","    for X, y in data_loader:\n","        X = X.to(device)\n","        y = y.to(device)\n","        output = model(X)\n","        loss = loss_function(output, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / num_batches\n","    print(f\"Train loss: {avg_loss}\")\n","\n","def test_model(data_loader, model, loss_function, best_val_loss):\n","    \n","    num_batches = len(data_loader)\n","    total_loss = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in data_loader:\n","            X = X.to(device)\n","            y = y.to(device)\n","            output = model(X)\n","            total_loss += loss_function(output, y).item()\n","\n","    avg_loss = total_loss / num_batches\n","    print(f\"Test loss: {avg_loss}\")\n","    if avg_loss < best_val_loss:\n","        best_val_loss = avg_loss\n","        torch.save(model.state_dict(), 'model_gru.pth')\n","        print('Save new best model')\n","    return best_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1xht__JJTJJ"},"outputs":[],"source":["best_val_loss = torch.inf\n","for ix_epoch in tqdm(range(100)):\n","    print(f\"Epoch {ix_epoch}\\n---------\")\n","    train_model(train_loader, model, loss_function, optimizer=optimizer)\n","    best_val_loss = test_model(val_loader, model, loss_function, best_val_loss)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"H1wq-damLoto"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGebRAKoLoto"},"outputs":[],"source":["def predict(data_loader, model):\n","    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n","    function.\n","    \"\"\"\n","    output = torch.tensor([])\n","    model.eval()\n","    with torch.no_grad():\n","        for X, _ in data_loader:\n","            X = X.to(device)\n","            y_star = model(X)\n","            output = torch.cat((output, y_star.detach().cpu()), 0)\n","    \n","    return output"]},{"cell_type":"code","source":["PATH = './model_gru.pth'\n","model.load_state_dict(torch.load(PATH))"],"metadata":{"id":"vehEFCADLotp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bd-gdyAYLotp"},"outputs":[],"source":["train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","\n","ystar_col = \"Model forecast\"\n","df_train[ystar_col] = predict(train_eval_loader, model).numpy()\n","df_val[ystar_col] = predict(val_loader, model).numpy()\n","df_test[ystar_col] = predict(test_loader, model).numpy()\n","\n","df_out = pd.concat((df_train, df_val, df_test))[[target, ystar_col]]\n","\n","for c in df_out.columns:\n","    df_out[c] = df_out[c] * target_stdev + target_mean\n","\n","print(df_out)"]},{"cell_type":"code","source":["import numpy as np\n","import math\n","from sklearn.metrics import mean_squared_error\n","\n","def MAPE(Y_actual,Y_Predicted):\n","    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n","    return mape\n","\n","print( 'MPAE =', MAPE(df_test['Open_lead1'], df_test['Model forecast']) )\n","print( 'RMSE =', math.sqrt(mean_squared_error(df_val['Open_lead1'], df_val['Model forecast'])) )"],"metadata":{"id":"-Ax1iMUtwUBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hM-41IhLotp"},"outputs":[],"source":["fig = px.line(df_out, labels={'value': \"Open\", 'created_at': 'Date'})\n","fig.add_vline(x=val_start, line_width=4, line_dash=\"dash\")\n","fig.add_vline(x=test_start, line_width=4, line_dash=\"dash\")\n","# fig.add_annotation(xref=\"paper\", x=0.75, yref=\"paper\", y=0.8, text=\"Test set start\", showarrow=False)\n","fig.update_layout(\n","  template=plot_template, legend=dict(orientation='h', y=1.02, title_text=\"\")\n",")\n","fig.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"eoQ5cv1GD550"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"afa1953de36fe74bf64e0007a050b01fd0993b8df6207b9778c7f34846b9bbf9"},"kernelspec":{"display_name":"Python 3.8.3 64-bit ('complete3.8': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"orig_nbformat":4,"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}